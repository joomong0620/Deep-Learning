{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 오차 역전파 Backpropagation\n",
    "오차 역전파는 출력층에서 입력층으로 오차를 전파하면서 가중치를 업데이트하는 방법\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**역사적 배경**\n",
    "1969년에 마빈 민스키(Marvin Minsky)와 세이무어 페퍼트(Seymour Papert)는 저서 Perceptrons에서 단층 퍼셉트론이 XOR 문제와 같은 선형적으로 구분할 수 없는 문제를 해결하지 못한다는 점을 지적했고, 이로 인해 신경망 연구는 잠시 정체기에 들어섰다.\n",
    "\n",
    "\n",
    "다층 신경망의 경우에는 학습이 잘 이루어지지 않았는데, 이는 다층 구조에서 오차를 효율적으로 전달하는 방법이 부족했기 때문이다.\n",
    "\n",
    "\n",
    "1986년에 인지과학자 데이비드 럼멜하트(David Rumelhart), 제프리 힌튼(Geoffrey Hinton), 로널드 윌리엄스(Ronald Williams)가 논문 \"Learning Representations by Back-Propagating Errors\"를 발표하면서 오차역전파 알고리즘을 대중화했다. 이 논문은 다층 신경망이 기울기를 통해 효과적으로 학습할 수 있도록 오차를 계층을 거꾸로 전달하는 방식을 설명했다.\n",
    "\n",
    "\n",
    "데이빗 럼멜하트 오차역전파 https://brunch.co.kr/@hvnpoet/70\n",
    "\n",
    "\n",
    "제프리 힌튼 볼츠만머신 CNN https://brunch.co.kr/@hvnpoet/46\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "https://machinelearningknowledge.ai/animated-explanation-of-feed-forward-neural-network-architecture/\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](https://d.pr/i/4nYlr4+)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "오차역전파에는 미분의 연쇄법칙이 적용되고 이는 마치 러시아의 전통인형 마트료시카와 비슷하다.\n",
    "\n",
    "\n",
    "![](https://www.sungyujin.co.kr/files/attach/images/166/811/001/3a4824b8d04766105bd016779daddbe1.jpg)\n"
   ],
   "id": "2a8a8667a6bbab46"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**오차역전파의 단계**\n",
    "1. **순전파(Forward Propagation)**: 입력 데이터가 네트워크를 통과하며 예측값을 생성한다.\n",
    "2. **오차 계산(Error Calculation)**: 예측값과 실제 목표값 사이의 오차를 계산한다. 대표적인 오차 함수는 평균 제곱 오차(MSE)이다.\n",
    "3. **오차 역전파(Backpropagation)**: 오차를 네트워크의 각 가중치로 전파하여 가중치를 조정한다.\n",
    "4. **가중치 갱신(Update Weights)**: 경사하강법을 통해 오차가 감소하도록 각 가중치를 갱신한다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**간단히 단일 계층(입력 $ x $ -> 은닉층 $ h $ -> 출력 $ y $) 신경망에서 오차역전파 설명**\n",
    "\n",
    "\n",
    "![](https://d.pr/i/juFjHc+)\n",
    "\n",
    "\n",
    "1. **순전파 단계**\n",
    "\n",
    "\n",
    "    - 가중치 $ w_1 $와 $ w_2 $가 각각 입력과 은닉층, 은닉층과 출력층 사이에 존재한다고 하자.\n",
    "    - 입력 $ x $가 은닉층 $ h $에 도달하면서 가중치 $ w_1 $을 곱한 뒤 활성화 함수를 적용:\n",
    "      $\n",
    "      h = f(x \\cdot w_1)\n",
    "      $\n",
    "    - 은닉층 출력 $ h $가 출력층 $ y $로 전달되며 가중치 $ w_2 $를 곱한 뒤 활성화 함수를 적용하여 최종 출력:\n",
    "      $\n",
    "      y = f(h \\cdot w_2)\n",
    "      $\n",
    "\n",
    "\n",
    "2. **오차 계산**\n",
    "\n",
    "\n",
    "    - 예측된 출력 $ y $와 목표 값 $ t $ 사이의 오차 $ E $를 계산한다. 여기서 평균 제곱 오차(MSE)를 사용하면:\n",
    "      $\n",
    "      E = \\frac{1}{2}(t - y)^2\n",
    "      $\n",
    "\n",
    "\n",
    "3. **오차의 기울기 계산**\n",
    "\n",
    "\n",
    "    - 오차 $ E $를 최소화하기 위해 각 가중치 $ w_1 $과 $ w_2 $에 대한 편미분을 구해야 한다.\n",
    "    - 출력층의 오차 기울기:\n",
    "      $\n",
    "      \\frac{\\partial E}{\\partial w_2} = \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial w_2}\n",
    "      $\n",
    "    - 은닉층의 오차 기울기:\n",
    "      $\n",
    "      \\frac{\\partial E}{\\partial w_1} = \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial h} \\cdot \\frac{\\partial h}{\\partial w_1}\n",
    "      $\n",
    "\n",
    "\n",
    "4. **가중치 갱신**\n",
    "\n",
    "\n",
    "    - 각 가중치는 학습률 $ \\eta $와 오차 기울기를 사용해 갱신한다:\n",
    "      $\n",
    "      w_2 = w_2 - \\eta \\cdot \\frac{\\partial E}{\\partial w_2}\n",
    "      $\n",
    "      $\n",
    "      w_1 = w_1 - \\eta \\cdot \\frac{\\partial E}{\\partial w_1}\n",
    "      $\n"
   ],
   "id": "1588d00ec83f52ce"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 순전파 / 역전파",
   "id": "70444a042aea7c4"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-01-15T02:47:56.165501Z",
     "start_time": "2026-01-15T02:47:52.899627Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T02:51:35.273705Z",
     "start_time": "2026-01-15T02:51:35.268084Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 단층 순전파/ 역전파\n",
    "def forward(x):\n",
    "    return x ** 2\n",
    "\n",
    "def backward(x):\n",
    "    \"\"\"forward함수의 도함수(기울기를 계산하는 식)\"\"\"\n",
    "    return 2 * x\n",
    "\n",
    "x = 3.0\n",
    "\n",
    "print(forward(x))\n",
    "\n",
    "# forward 함수를 x로 미분하면?\n",
    "# x가 변할때, forward 함수는 얼마나 변할까?\n",
    "print(backward(x))\n"
   ],
   "id": "14d85c03960fdf07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n",
      "6.0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T03:00:52.427574Z",
     "start_time": "2026-01-15T03:00:52.418662Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 다층 순전파/역전파\n",
    "layer1 = lambda x: x ** 2\n",
    "layer2 = lambda y: 2 * y\n",
    "\n",
    "def forward(x):\n",
    "    y = layer1(x)\n",
    "    z = layer2(y)\n",
    "    return z\n",
    "\n",
    "\n",
    "layer2_derivative = lambda y : 2\n",
    "layer1_derivative = lambda x : 2 * x\n",
    "\n",
    "def backward(x):\n",
    "    \"\"\"forward함수의 도함수\"\"\"\n",
    "    dz_dy = layer2_derivative(x) # layer2 도함수\n",
    "    dy_dx = layer1_derivative(x) # layer1 도함수\n",
    "    dz_dx = dz_dy * dy_dx\n",
    "    return dz_dx\n",
    "\n",
    "x = 3.0\n",
    "print(forward(x))\n",
    "\n",
    "# forward 함수(z)를 x로 미분하면?\n",
    "# x가 변할때 forward 함수(z)는 얼마나 변할까?\n",
    "print(backward(x))"
   ],
   "id": "7af9cf1134aa8ebf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.0\n",
      "12.0\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T03:04:39.886053Z",
     "start_time": "2026-01-15T03:04:39.867653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# torch\n",
    "# 다층 순전파/역전파\n",
    "layer1 = lambda x: x ** 2\n",
    "layer2 = lambda y: 2 * y\n",
    "\n",
    "def forward(x):\n",
    "    y = layer1(x)\n",
    "    z = layer2(y)\n",
    "    return z\n",
    "\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "z = forward(x)\n",
    "print(z)\n",
    "\n",
    "z.backward()\n",
    "print(x.grad.item())"
   ],
   "id": "ba57228319c37c07",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18.], grad_fn=<MulBackward0>)\n",
      "12.0\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 손실함수/활성화함수의 도함수\n",
    "\n",
    "\n",
    "### 손실 함수 (Loss Functions)의 도함수\n",
    "\n",
    "\n",
    "| 손실 함수 | 공식 | 도함수 | 비고 |\n",
    "|-----------|------|--------|------|\n",
    "| **MSELoss** (Mean Squared Error) | $L = \\frac{1}{n} \\sum (y - \\hat{y})^2$ | $\\frac{dL}{d\\hat{y}} = \\frac{2}{n} (\\hat{y} - y)$ | 예측값과 정답의 차이를 제곱해 평균, **이상치에 민감함** |\n",
    "| **L1Loss** (Mean Absolute Error) | $L = \\frac{1}{n} \\sum \\|y - \\hat{y}\\|$ | $\\frac{dL}{d\\hat{y}} = \\frac{1}{n} \\cdot \\text{sign}(\\hat{y} - y)$ | 절댓값 오차 평균, **이상치에 덜 민감**하지만 미분 불연속 |\n",
    "| **HuberLoss** | $L = \\begin{cases} \\frac{1}{2}(y - \\hat{y})^2 & \\text{if } \\|y - \\hat{y}\\| \\leq \\delta \\\\ \\delta \\cdot (\\|y - \\hat{y}\\| - \\frac{1}{2} \\delta) & \\text{otherwise} \\end{cases}$ | $\\frac{dL}{d\\hat{y}} = \\begin{cases} \\hat{y} - y & \\text{if } \\|\\hat{y} - y\\| \\leq \\delta \\\\ \\delta \\cdot \\text{sign}(\\hat{y} - y) & \\text{otherwise} \\end{cases}$ | MSE와 MAE의 장점 결합, **이상치에 덜 민감하면서 부드러운 미분** |\n",
    "| **BCELoss** (Binary Cross Entropy) | $L = - \\left( y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right)$ | $\\frac{dL}{d\\hat{y}} = -\\left( \\frac{y}{\\hat{y}} - \\frac{1 - y}{1 - \\hat{y}} \\right)$ | **이진 분류**에서 사용, 출력에 **sigmoid**를 적용해야 함 |\n",
    "| **BCEWithLogitsLoss** | $L = \\max(z, 0) - z \\cdot y + \\log(1 + e^{-\\|z\\|})$ | $\\frac{dL}{dz} = \\sigma(z) - y$ | sigmoid + BCELoss 결합 형태, **수치적으로 안정적** |\n",
    "| **CrossEntropyLoss** | $L = - \\log \\left( \\frac{e^{z_y}}{\\sum_j e^{z_j}} \\right )$ | $\\frac{dL}{dz_i} = \\text{softmax}(z_i) - y_i$ | 다중 클래스 분류용, 내부에서 softmax 포함, **출력에 softmax 불필요** |\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### 활성화 함수 (Activation Functions)의 도함수\n",
    "\n",
    "\n",
    "| 함수 이름 | 공식 | 도함수 | 비고 |\n",
    "|-----------|------|--------|------|\n",
    "| **Sigmoid** | $\\sigma(x) = \\frac{1}{1 + e^{-x}}$ | $\\sigma'(x) = \\sigma(x)(1 - \\sigma(x))$ | 이진 분류 확률 출력에 주로 사용 |\n",
    "| **Softmax** | $\\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$ | $\\frac{\\partial s_i}{\\partial x_j} = s_i (\\delta_{ij} - s_j)$ | 다중 클래스 확률 출력, 출력 간 상호작용 있음 |\n",
    "| **ReLU** | $ReLU(x) = \\max(0, x)$ | $ReLU'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ 0 & \\text{otherwise} \\end{cases}$ | 계산 간단하고 많이 사용됨 |\n",
    "| **LeakyReLU** | $LeakyReLU(x) = \\begin{cases} x & \\text{if } x > 0 \\\\ \\alpha x & \\text{otherwise} \\end{cases}$ | $LeakyReLU'(x) = \\begin{cases} 1 & \\text{if } x > 0 \\\\ \\alpha & \\text{otherwise} \\end{cases}$ | 음수 영역도 일부 통과시켜 죽은 ReLU 문제 완화 |\n",
    "| **Tanh** | $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$ | $\\tanh'(x) = 1 - \\tanh^2(x)$ | 출력이 -1~1 범위로 중심 정규화 효과 |\n",
    "| **Softmax + CrossEntropy** | $L = -\\sum_i y_i \\log(s_i)$ | $\\frac{\\partial L}{\\partial x_i} = s_i - y_i$ | softmax와 cross-entropy를 결합한 경우, 도함수가 매우 간단해짐 |\n"
   ],
   "id": "2a7df3229e090d38"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 선형층-Sigmoid-BCELoss의 역전파\n",
    "\n",
    "\n",
    "**순전파 (Forward)**\n",
    "\n",
    "\n",
    "1. **선형 결합:**\n",
    "   $z = wx + b$\n",
    "\n",
    "\n",
    "2. **시그모이드 활성화 함수:**\n",
    "   $\\hat{y} = \\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "\n",
    "3. **이진 크로스 엔트로피 손실 (Binary Cross Entropy):**\n",
    "   $L = -[y \\cdot \\log(\\hat{y}) + (1 - y) \\cdot \\log(1 - \\hat{y})]$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**역전파 (Backward)**\n",
    "\n",
    "\n",
    "1. **BCE 손실의 출력에 대한 도함수:**\n",
    "   $\\frac{\\partial L}{\\partial \\hat{y}} = -\\left( \\frac{y}{\\hat{y}} - \\frac{1 - y}{1 - \\hat{y}} \\right)$\n",
    "\n",
    "\n",
    "2. **시그모이드 함수의 도함수:**\n",
    "   $\\frac{\\partial \\hat{y}}{\\partial z} = \\hat{y}(1 - \\hat{y})$\n",
    "\n",
    "\n",
    "3. **연쇄법칙 (Chain Rule):**\n",
    "\n",
    "\n",
    "   * 출력에서 z까지:\n",
    "\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial z} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z}\n",
    "     $$\n",
    "\n",
    "\n",
    "   * z에서 w, b에 대해:\n",
    "\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w} = \\frac{\\partial L}{\\partial z} \\cdot x\n",
    "     $$\n",
    "\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial b} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial b} = \\frac{\\partial L}{\\partial z} \\cdot 1\n",
    "     $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "* $dL/dw = \\left( \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\right) \\cdot x$\n",
    "* $dL/db = \\left( \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z} \\right) \\cdot 1$\n"
   ],
   "id": "80df9c1191923d86"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T03:49:28.857128Z",
     "start_time": "2026-01-15T03:49:28.847700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 선형층-Sigmoid-BCELoss\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def bce_loss(y_hat, y):\n",
    "    # - ((y=1일때 손실) + (y=0일때 손실))\n",
    "    return -((y * np.log(y_hat)) + ((1 - y) * np.log(1 - y_hat)))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"시그모이드 함수의 도함수\"\"\"\n",
    "    p = sigmoid(x)\n",
    "    return p*(1 - p)\n",
    "\n",
    "def bce_loss_derivative(y_hat, y):\n",
    "    \"\"\"bce_loss 함수의 도함수\"\"\"\n",
    "    # -((y=1일때 기울기) + (y=0일때 기울기))\n",
    "    return -((y / y_hat) - ((1 - y) / (1 - y_hat)))\n",
    "\n",
    "\n",
    "# 변수 선언\n",
    "x = 3.5 # 입력\n",
    "y = 0   # 타겟(정답)\n",
    "w = 2.0 # 출력층 가중치\n",
    "b = 1.0 # 출력층 절편\n",
    "\n",
    "\n",
    "# 순전파\n",
    "z = w * x + b # 출력층의 출력(logit)\n",
    "y_hat = sigmoid(z) # 이진분류 출력층 활성화 함수(확률값)\n",
    "loss = bce_loss(y_hat, y) # 이진분류 손실함수\n",
    "print(f'순전파 : y_hat={y_hat}, loss={loss}')\n",
    "\n",
    "\n",
    "# 역전파\n",
    "dL_dyhat = bce_loss_derivative(y_hat, y)\n",
    "dyhat_dz = sigmoid_derivative(z)\n",
    "\n",
    "\n",
    "dz_dw = x\n",
    "dz_db = 1\n",
    "\n",
    "dL_dz = dL_dyhat * dyhat_dz\n",
    "dL_dw = dL_dz * dz_dw\n",
    "dL_db = dL_dz * dz_db\n",
    "\n",
    "print(f'역전파 : w기울기 ={dL_dw}')\n",
    "print(f'역전파 : b기울기 ={dL_db}')\n"
   ],
   "id": "15e35018afbb8b4a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "순전파 : y_hat=0.9996646498695336, loss=8.000335406373212\n",
      "역전파 : w기울기 =3.4988262745433674\n",
      "역전파 : b기울기 =0.9996646498695335\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T03:58:22.434633Z",
     "start_time": "2026-01-15T03:58:22.426894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# torch 버전\n",
    "# 1. 변수 선언\n",
    "x = torch.tensor(3.5) # 입력\n",
    "y = torch.tensor(0.0)   # 타겟(정답)\n",
    "w = torch.tensor(2.0, requires_grad=True) # 출력층 가중치(미분 대상)\n",
    "b = torch.tensor(1.0, requires_grad=True) # 출력층 절편(미분 대상)\n",
    "\n",
    "# 2. 순전파\n",
    "# - F.sigmoid(z)\n",
    "# - F.binary_cross_entropy(y_hat, y)\n",
    "z = w * x + b\n",
    "y_hat = F.sigmoid(z)\n",
    "loss = F.binary_cross_entropy(y_hat, y)\n",
    "print(f'순전파 : y_hat={y_hat}, loss={loss}')\n",
    "\n",
    "# 3. 역전파 (기울기 계산)\n",
    "loss.backward()\n",
    "\n",
    "# 4. w/b 기울기 확인\n",
    "print(f'역전파 : w기울기={w.grad.item()}')\n",
    "print(f'역전파 : b기울기={b.grad.item()}')"
   ],
   "id": "46f35f69c31d6e5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "순전파 : y_hat=0.9996646642684937, loss=8.000378608703613\n",
      "역전파 : w기울기=3.498826265335083\n",
      "역전파 : b기울기=0.9996646642684937\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 출력층-Softmax-CrossEntropyLoss의 역전파\n",
    "\n",
    "\n",
    "softmax 함수와 cross-entropy loss를 **함께 사용할 때**의 도함수는 다음과 같이 **아주 간단한 형태**로 정리된다:\n",
    "\n",
    "\n",
    "**1. 전제**\n",
    "\n",
    "\n",
    "* 모델 출력 (로짓): **x**\n",
    "* softmax 출력:\n",
    "\n",
    "\n",
    "  $$\n",
    "  s_i = \\frac{e^{x_i}}{\\sum_j e^{x_j}}\n",
    "  $$\n",
    "* 정답 라벨 (원-핫 인코딩): **y**\n",
    "* cross-entropy loss:\n",
    "\n",
    "\n",
    "  $$\n",
    "  L = -\\sum_i y_i \\log(s_i)\n",
    "  $$\n",
    "\n",
    "\n",
    "**2. 도함수 결과**\n",
    "\n",
    "\n",
    "softmax와 cross-entropy를 **연속으로 쓴 뒤 미분하면**,\n",
    "**놀랍게도** 도함수는 이렇게 단순해진다:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_i} = s_i - y_i\n",
    "$$\n",
    "\n",
    "\n",
    "즉,\n",
    "\n",
    "\n",
    "* $s_i$: softmax 확률 출력\n",
    "* $y_i$: 정답값 (one-hot이면 0 또는 1)"
   ],
   "id": "d2487c14ba14405b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T05:49:03.428051Z",
     "start_time": "2026-01-15T05:49:03.411203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 선형출력층 - softmax - cross_entropy_loss\n",
    "def softmax(x, axis=1):\n",
    "    exp_x = np.exp(x - np.max(x, axis=axis)) # overflow방지 (값안정화)\n",
    "    return exp_x / np.sum(exp_x, axis=axis)\n",
    "\n",
    "def cross_entropy_loss(y_hat, y_true_index):\n",
    "    print(y_hat.shape) # (n, n_classes)\n",
    "    return np.mean([-np.log(y_hat_[y_true_index]) for y_hat_ in y_hat])\n",
    "\n",
    "def cross_entropy_loss_derivative(y_hat, y_true_index):\n",
    "    \"\"\"\n",
    "    cross_entory_loss, softmax 통합된 도함수\n",
    "    \"\"\"\n",
    "    grad = y_hat.copy()\n",
    "    grad[:, y_true_index] -= 1\n",
    "    return grad\n",
    "\n",
    "# 변수\n",
    "X = np.array([[2.0, -1.0]]) # batch 차원\n",
    "y_true_index = 0 # 0 1 2 클래스 중에서 0번지 클래스\n",
    "# 입력2 -> 출력3 : W.shape(3,2), b.shape(3,)\n",
    "W = np.array([[0.1, 0.3],\n",
    "              [0.2, -0.2],\n",
    "              [-0.1, 0.4]])\n",
    "\n",
    "b = np.array([0.0, 0.0, 0.0])\n",
    "\n",
    "\n",
    "# 순전파\n",
    "z = X @ W.T + b\n",
    "y_hat = softmax(z, axis=1)\n",
    "loss = cross_entropy_loss(y_hat, y_true_index)\n",
    "print(f'순전파: y_hat={y_hat}, loss={loss}')\n",
    "\n",
    "\n",
    "# 역전파\n",
    "dL_dz = cross_entropy_loss_derivative(y_hat, y_true_index)\n",
    "print(dL_dz.shape)\n",
    "\n",
    "# 도함수 도출\n",
    "dz_dW = X\n",
    "dz_db = 1\n",
    "\n",
    "\n",
    "dL_dW = np.outer(dL_dz, dz_dW) # (3,) (2,) 곱해서 (3, 2)\n",
    "dL_db = dL_dz * dz_db # 서로 달라서 곱셈할 수 없음\n",
    "\n",
    "\n",
    "print(f'역전파 : w기울기={dL_dW}') # 3행 2열의 결과가 나옴\n",
    "print(f'역전파 : b기울기={dL_db}')\n"
   ],
   "id": "8ead6355c81df0d8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 3)\n",
      "순전파: y_hat=[[0.27622147 0.55624174 0.16753679]], loss=1.2865523010014777\n",
      "(1, 3)\n",
      "역전파 : w기울기=[[-1.44755706  0.72377853]\n",
      " [ 1.11248347 -0.55624174]\n",
      " [ 0.33507358 -0.16753679]]\n",
      "역전파 : b기울기=[[-0.72377853  0.55624174  0.16753679]]\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# np.outer() 두 행렬의 외적 -> 행렬\n",
    "a = np.array([1, 2])  # (2,)\n",
    "b = np.array([10, 20, 30]) # (3, )\n",
    "c = np.outer(a, b) # (2,3)\n",
    "print(c.shape)\n",
    "\n",
    "# np.dot() 두행렬 내적 -> 스칼라\n",
    "d = np.array([3, 4]) # (2,)\n",
    "e = np.dot(a, d)\n",
    "print(e)"
   ],
   "id": "96af22012202cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T06:05:36.023466Z",
     "start_time": "2026-01-15T06:05:36.002830Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# torch 자동미분\n",
    "# 변수\n",
    "X = torch.tensor([[2.0, -1.0]])\n",
    "y_true_index = torch.tensor([0], dtype=torch.long)\n",
    "W = torch.tensor([[0.1, 0.3],\n",
    "                  [0.2, -0.2],\n",
    "                  [-0.1, 0.4]], requires_grad=True)\n",
    "b = torch.tensor([0.0, 0.0, 0.0], requires_grad=True)\n",
    "\n",
    "# 순전파\n",
    "# - F.softmax()\n",
    "# - F.cross_entropy(y_hat, y_true_index)\n",
    "x = X @ W.T + b\n",
    "y_hat = F.softmax(x, dim=1)\n",
    "loss = F.cross_entropy(y_hat, y_true_index) # cross_entropy에는 선형출력층 결과(z)를 전달해야한다!\n",
    "print(f'순전파: y_hat={y_hat}, loss={loss}')\n",
    "\n",
    "# 역전파\n",
    "loss.backward()\n",
    "\n",
    "# w,b 기울기 확인\n",
    "print(f'역전파 : w기울기={w.grad}')\n",
    "print(f'역전파 : b기울기={b.grad}')\n",
    "\n",
    "\n"
   ],
   "id": "8db86d5cc5cd804c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "순전파: y_hat=tensor([[0.2762, 0.5562, 0.1675]], grad_fn=<SoftmaxBackward0>), loss=1.1694340705871582\n",
      "역전파 : w기울기=3.498826265335083\n",
      "역전파 : b기울기=tensor([-0.2139,  0.1814,  0.0325])\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 선형층-ReLU-MSELoss의 역전파\n",
    "\n",
    "\n",
    "**1. 구조 요약**\n",
    "\n",
    "\n",
    "* 입력: $x$\n",
    "* 은닉층 선형: $z\\_1 = w\\_1 \\cdot x + b\\_1$\n",
    "* 은닉층 활성화: $h = \\text{ReLU}(z\\_1)$\n",
    "* 출력층 선형: $z\\_2 = w\\_2 \\cdot h + b\\_2$\n",
    "* 예측값: $\\hat{y} = z\\_2$\n",
    "* 손실: $L = (\\hat{y} - y)^2$\n",
    "\n",
    "\n",
    "**2. 순전파**\n",
    "\n",
    "\n",
    "1. **은닉층 선형 계산**\n",
    "   $z\\_1 = w\\_1 \\cdot x + b\\_1$\n",
    "\n",
    "\n",
    "2. **ReLU 적용**\n",
    "   $h = \\text{ReLU}(z\\_1) = \\max(0, z\\_1)$\n",
    "\n",
    "\n",
    "3. **출력층 선형 계산**\n",
    "   $z\\_2 = w\\_2 \\cdot h + b\\_2$\n",
    "\n",
    "\n",
    "4. **예측값 및 손실**\n",
    "   $\\hat{y} = z\\_2$\n",
    "   $L = (\\hat{y} - y)^2 = (z\\_2 - y)^2$\n",
    "\n",
    "\n",
    "**5. 역전파 (Chain Rule)**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(1) 출력층 가중치 $w\\_2$, 편향 $b\\_2$\n",
    "\n",
    "\n",
    "* $\\frac{\\partial L}{\\partial \\hat{y}} = 2 (\\hat{y} - y)$\n",
    "* $\\frac{\\partial \\hat{y}}{\\partial z\\_2} = 1$\n",
    "* $\\frac{\\partial z\\_2}{\\partial w\\_2} = h$\n",
    "* $\\frac{\\partial z\\_2}{\\partial b\\_2} = 1$\n",
    "\n",
    "\n",
    "**연쇄법칙 적용:**\n",
    "\n",
    "\n",
    "* $\\frac{\\partial L}{\\partial w\\_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z\\_2} \\cdot \\frac{\\partial z\\_2}{\\partial w\\_2} = 2 (\\hat{y} - y) \\cdot h$\n",
    "\n",
    "\n",
    "* $\\frac{\\partial L}{\\partial b\\_2} = 2 (\\hat{y} - y) \\cdot 1 = 2 (\\hat{y} - y)$\n",
    "\n",
    "\n",
    "(2) 은닉층 가중치 $w\\_1$, 편향 $b\\_1$\n",
    "\n",
    "\n",
    "* $\\frac{\\partial L}{\\partial z\\_2} = \\frac{\\partial L}{\\partial \\hat{y}} \\cdot \\frac{\\partial \\hat{y}}{\\partial z\\_2} = 2 (\\hat{y} - y)$\n",
    "* $\\frac{\\partial z\\_2}{\\partial h} = w\\_2$\n",
    "* $\\frac{\\partial h}{\\partial z\\_1} = \\text{ReLU}'(z\\_1) = \\begin{cases} 1 & z\\_1 > 0 \\ 0 & \\text{else} \\end{cases}$\n",
    "* $\\frac{\\partial z\\_1}{\\partial w\\_1} = x$\n",
    "* $\\frac{\\partial z\\_1}{\\partial b\\_1} = 1$\n",
    "\n",
    "\n",
    "**연쇄법칙 적용:**\n",
    "\n",
    "\n",
    "* $\\frac{\\partial L}{\\partial z\\_1} = \\frac{\\partial L}{\\partial z\\_2} \\cdot \\frac{\\partial z\\_2}{\\partial h} \\cdot \\frac{\\partial h}{\\partial z\\_1} = 2 (\\hat{y} - y) \\cdot w\\_2 \\cdot \\text{ReLU}'(z\\_1)$\n",
    "\n",
    "\n",
    "* $\\frac{\\partial L}{\\partial w\\_1} = \\frac{\\partial L}{\\partial z\\_1} \\cdot \\frac{\\partial z\\_1}{\\partial w\\_1} = 2 (\\hat{y} - y) \\cdot w\\_2 \\cdot \\text{ReLU}'(z\\_1) \\cdot x$\n",
    "\n",
    "\n",
    "* $\\frac{\\partial L}{\\partial b\\_1} = 2 (\\hat{y} - y) \\cdot w\\_2 \\cdot \\text{ReLU}'(z\\_1) \\cdot 1$"
   ],
   "id": "ea5fbf9dd26479ab"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T06:48:32.616939Z",
     "start_time": "2026-01-15T06:48:32.602046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def mse_loss(y_hat, y):\n",
    "    return np.mean((y_hat - y) ** 2)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "def mse_loss_derivative(y_hat, y):\n",
    "    return 2 * (y_hat - y)\n",
    "\n",
    "# 변수선언\n",
    "x = np.array([3.5])\n",
    "y = np.array([10.0])\n",
    "\n",
    "w1 = np.array([2.0])\n",
    "b1 = np.array([1.0])\n",
    "\n",
    "w2 = np.array([1.5])\n",
    "b2 = np.array([0.5])\n",
    "\n",
    "# 순전파\n",
    "z1 = w1 * x + b1\n",
    "h = relu(z1)\n",
    "z2 = w2 * h + b2 # 예측값\n",
    "loss = mse_loss(z2, y)\n",
    "print(f'순전파: y_hat={z2}, loss={loss}')\n",
    "\n",
    "\n",
    "\n",
    "# 역전파\n",
    "# - 출력층 w2, b2\n",
    "dL_dz2 = mse_loss_derivative(z2, y)\n",
    "\n",
    "dz2_dw2 = h\n",
    "dz2_db2 = 1\n",
    "\n",
    "dL_dw2 = dL_dz2 * dz2_dw2\n",
    "dL_db2 = dL_dz2 * dz2_db2\n",
    "\n",
    "# w2, b2 기울기\n",
    "print(f'출력층 w2 기울기: {dL_dw2}')\n",
    "print(f'출력층 b2 기울기: {dL_db2}')\n",
    "\n",
    "\n",
    "# 은닉층 w1, b1\n",
    "dz2_dh = w2\n",
    "dh_dz1 = relu_derivative(z1)\n",
    "\n",
    "dL_dz1 = dL_dz2 * dz2_dh * dh_dz1\n",
    "dz1_dw1 = x\n",
    "dz1_db1 = 1\n",
    "\n",
    "dL_dw1 = dL_dz1 * dz1_dw1\n",
    "dL_db1 = dL_dz1 * dz1_db1\n",
    "\n",
    "\n",
    "# w1, b1 기울기\n",
    "print(f'은닉층 w1 기울기: {dL_dw1}')\n",
    "print(f'은닉층 b1 기울기 : {dL_db1}')\n"
   ],
   "id": "f065d8a68b3a7c34",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "순전파: y_hat=[12.5], loss=6.25\n",
      "출력층 w2 기울기: [40.]\n",
      "출력층 b2 기울기: [5.]\n",
      "은닉층 w1 기울기: [26.25]\n",
      "은닉층 b1 기울기 : [7.5]\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-15T06:55:58.105445Z",
     "start_time": "2026-01-15T06:55:58.085995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# torch 자동미분\n",
    "# 변수선언\n",
    "x = torch.tensor([3.5])\n",
    "y = torch.tensor([10.0])\n",
    "\n",
    "w1 = torch.tensor([2.0], requires_grad=True)\n",
    "b1 = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "w2 = torch.tensor([1.5], requires_grad=True)\n",
    "b2 = torch.tensor([0.5], requires_grad=True)\n",
    "\n",
    "# 순전파\n",
    "z1 = w1 * x + b1\n",
    "h = F.relu(z1)\n",
    "z2 = w2 * h + b2 # 예측값\n",
    "loss = F.mse_loss(z2, y)\n",
    "print(f'순전파: y_hat={z2}, loss={loss}')\n",
    "\n",
    "\n",
    "# 역전파\n",
    "loss.backward()\n",
    "\n",
    "# 기울기/절편\n",
    "print(f'출력층 w2 기울기 : {w2.grad}')\n",
    "print(f'출력층 b2 기울기 : {b2.grad}')\n",
    "\n",
    "print(f'은닉층 w1 기울기: {dL_dw1}')\n",
    "print(f'은닉층 b1 기울기 : {dL_db1}')\n"
   ],
   "id": "bd2e1ff10668eed0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "순전파: y_hat=tensor([12.5000], grad_fn=<AddBackward0>), loss=6.25\n",
      "출력층 w2 기울기 : tensor([40.])\n",
      "출력층 b2 기울기 : tensor([5.])\n",
      "은닉층 w1 기울기: [26.25]\n",
      "은닉층 b1 기울기 : [7.5]\n"
     ]
    }
   ],
   "execution_count": 44
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
